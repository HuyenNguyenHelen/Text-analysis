# PDF downloading
## Extract metadata :\\
## Fix forbidden errors
## Building a program for downloading file PDF, using id
import ssl
from bs4 import BeautifulSoup
import urllib.request
import requests

ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE


f_id =urllib.request.urlopen("https://digital.library.unt.edu/search/?t=fulltext&t1=dc_title&t2=dc_description&t3=untl_agent&t4=dc_publisher&q5=%28information+science%29&t5=dc_subject&t6=dc_coverage&t7=dc_identifier&t8=page&sort=default&searchType=advanced&fq=untl_collection%3AUNTETD&fq=untl_decade%3A2010-2019&fq=str_degree_level%3ADoctoral")
webpage = f_id.read()
soup = BeautifulSoup(webpage)
#print(soup)
id_list = []
for i in range (1, 25):
  for tag in soup.find_all('a', id="result-title-"+str(i)):
    id_list.append(tag.get("data-meta-id"))
#print(id_list)

parse_link_begin = "https://digital.library.unt.edu/ark:/67531/"
parse_link_end = "?q=information%20science"
f_link=[]
for id in id_list:
  f_link.append(parse_link_begin+id+parse_link_end)
#print(f_link)
for link in f_link:
  open_link = urllib.request.urlopen(link)
  webpage = open_link.read()
  soup = BeautifulSoup(webpage)
  #print(soup)
#all_links = soup.find_all("a")

list_pdf_link = []
for link in soup.select ("a[href$='.pdf']"):
  list_pdf_link.append(link.get("href"))
for l in list_pdf_link:
  response = requests.get(l, allow_redirects=True)
  for i in range (1,3):
    with open('E:\webscrapping\dissertation' + str(i)+'.pdf', 'wb') as f:
      f.write(response.content)
    f.closed
